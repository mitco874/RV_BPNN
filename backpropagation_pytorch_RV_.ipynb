{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33VbFKZLvHro"
      },
      "source": [
        "# Loading Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "mddDyqHhvHrs"
      },
      "outputs": [],
      "source": [
        "import torch, pandas, scipy \n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from csv import reader\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-PX4kGcvHru"
      },
      "source": [
        "# Pre procesamiento de datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "5Tv2tW7ivHru"
      },
      "outputs": [],
      "source": [
        "# cargar archivo csv\n",
        "def load_csv(filename):\n",
        "    dataset = list()\n",
        "    with open(filename, 'r') as file:\n",
        "        csv_reader = reader(file)\n",
        "        for row in csv_reader:\n",
        "            if not row:\n",
        "                continue\n",
        "            dataset.append(row)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "ludJkxgivHrv"
      },
      "outputs": [],
      "source": [
        "# convertir los valores de string a float.\n",
        "def str_column_to_float(dataset, column):\n",
        "    for row in dataset:\n",
        "        row[column] = float(row[column].strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "th4PJnUNvHrv"
      },
      "outputs": [],
      "source": [
        "# convertir valores string a enteros.\n",
        "def str_column_to_int(dataset, column):\n",
        "    class_values = [row[column] for row in dataset]\n",
        "    unique = set(class_values)\n",
        "    lookup = dict()\n",
        "    for i, value in enumerate(unique):\n",
        "        lookup[value] = i\n",
        "    for row in dataset:\n",
        "            row[column] = lookup[row[column]]\n",
        "    return lookup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lectura de datos del dataset**"
      ],
      "metadata": {
        "id": "Yxo6MEBez7UA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#data = pd.read_csv('QCM10.csv',encoding = \"ISO-8859-1\")\n",
        "#data.head()"
      ],
      "metadata": {
        "id": "E9RXailIwZoE"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFl2sD1jvHrw",
        "outputId": "e34143ee-560c-4df0-cd5d-17b4c50b66ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ -11.9800,  -10.9900,  -19.1200,  -17.2800,  -33.1300,  -28.4500,\n",
            "          -48.8300,  -40.7700,  -62.4900,  -50.8200,    1.0000,    0.0000,\n",
            "            0.0000,    0.0000],\n",
            "        [ -12.1500,  -11.3300,  -22.3300,  -19.9500,  -39.8200,  -33.6400,\n",
            "          -56.9000,  -46.7700,  -73.3200,  -58.9600,    1.0000,    0.0000,\n",
            "            0.0000,    0.0000],\n",
            "        [ -12.5800,  -11.7400,  -26.6700,  -23.3400,  -46.4800,  -38.6900,\n",
            "          -65.9500,  -53.4600,  -84.5300,  -67.2100,    1.0000,    0.0000,\n",
            "            0.0000,    0.0000],\n",
            "        [ -13.7900,  -12.8200,  -30.5600,  -26.1800,  -52.3000,  -42.9800,\n",
            "          -73.8100,  -59.1900,  -94.4100,  -74.4000,    1.0000,    0.0000,\n",
            "            0.0000,    0.0000],\n",
            "        [ -15.7300,  -13.8700,  -34.5400,  -28.6500,  -57.4400,  -46.2600,\n",
            "          -80.3700,  -63.4900, -102.9400,  -80.2500,    1.0000,    0.0000,\n",
            "            0.0000,    0.0000],\n",
            "        [ -58.4100,  -38.6600,  -83.5800,  -57.3300, -110.2400,  -76.8000,\n",
            "         -133.7700,  -96.1300, -171.0500, -124.1500,    0.0000,    1.0000,\n",
            "            0.0000,    0.0000],\n",
            "        [ -59.6800,  -40.0100,  -84.7500,  -58.7500, -109.9100,  -77.2800,\n",
            "         -130.6500,  -94.6900, -161.8100, -118.7000,    0.0000,    1.0000,\n",
            "            0.0000,    0.0000],\n",
            "        [ -60.1200,  -40.7300,  -85.1900,  -59.4700, -108.0800,  -76.4300,\n",
            "         -127.5100,  -92.8800, -155.5400, -114.7600,    0.0000,    1.0000,\n",
            "            0.0000,    0.0000],\n",
            "        [ -60.4100,  -41.2000,  -84.7300,  -59.3100, -106.4600,  -75.3900,\n",
            "         -124.6200,  -90.8800, -151.5600, -112.5800,    0.0000,    1.0000,\n",
            "            0.0000,    0.0000],\n",
            "        [ -60.8700,  -39.8800,  -84.2200,  -57.5800, -105.3200,  -73.3600,\n",
            "         -122.5900,  -88.2800, -146.9900, -109.5200,    0.0000,    1.0000,\n",
            "            0.0000,    0.0000],\n",
            "        [ -59.4100,  -61.8600,  -91.6500,  -81.0200, -131.0500, -105.5100,\n",
            "         -165.6800, -126.3000, -207.1600, -153.4400,    0.0000,    0.0000,\n",
            "            1.0000,    0.0000],\n",
            "        [ -61.1600,  -62.9200,  -93.5500,  -82.3900, -133.1100, -107.2700,\n",
            "         -168.6200, -128.9400, -210.8500, -157.0100,    0.0000,    0.0000,\n",
            "            1.0000,    0.0000],\n",
            "        [ -61.8000,  -62.9900,  -94.1500,  -82.4500, -133.2400, -107.1500,\n",
            "         -165.3300, -126.5500, -204.6000, -152.8100,    0.0000,    0.0000,\n",
            "            1.0000,    0.0000],\n",
            "        [ -61.9400,  -62.7800,  -94.4600,  -82.3900, -131.5400, -105.6000,\n",
            "         -160.7400, -123.0800, -195.4900, -146.2600,    0.0000,    0.0000,\n",
            "            1.0000,    0.0000],\n",
            "        [ -62.3400,  -61.8700,  -95.0900,  -81.5300, -128.2300, -102.0500,\n",
            "         -156.3100, -119.7100, -187.9500, -140.7900,    0.0000,    0.0000,\n",
            "            1.0000,    0.0000],\n",
            "        [ -44.1000,  -35.0200,  -66.1300,  -50.4300,  -90.2900,  -68.0000,\n",
            "         -101.7400,  -76.5300, -117.6700,  -88.6300,    0.0000,    0.0000,\n",
            "            0.0000,    1.0000],\n",
            "        [ -44.6700,  -35.3900,  -66.6200,  -50.8400,  -84.4100,  -63.9400,\n",
            "          -97.1100,  -73.3500, -111.0600,  -83.9800,    0.0000,    0.0000,\n",
            "            0.0000,    1.0000],\n",
            "        [ -44.5000,  -35.2700,  -66.6000,  -50.9800,  -80.7900,  -61.2400,\n",
            "          -93.3400,  -70.6700, -106.1800,  -80.4500,    0.0000,    0.0000,\n",
            "            0.0000,    1.0000],\n",
            "        [ -44.3400,  -34.9700,  -66.5800,  -50.9300,  -78.0700,  -59.2700,\n",
            "          -89.9500,  -68.1800, -101.4600,  -77.0600,    0.0000,    0.0000,\n",
            "            0.0000,    1.0000],\n",
            "        [ -44.8000,  -35.1700,  -66.2000,  -50.4200,  -76.8000,  -58.2600,\n",
            "          -87.0900,  -65.5500,  -98.1400,  -73.8900,    0.0000,    0.0000,\n",
            "            0.0000,    1.0000],\n",
            "        [ -52.2200,  -41.6700,  -75.7400,  -57.7000, -109.1000,  -83.4200,\n",
            "         -149.6200, -114.9600, -197.3500, -151.9200,    0.0000,    0.0000,\n",
            "            0.0000,    0.0000],\n",
            "        [ -54.1500,  -42.9100,  -77.4300,  -59.3300, -112.0400,  -86.1600,\n",
            "         -155.0400, -119.5600, -205.7700, -159.3000,    0.0000,    0.0000,\n",
            "            0.0000,    0.0000],\n",
            "        [ -54.7200,  -43.0400,  -78.1900,  -60.0500, -113.3600,  -87.5000,\n",
            "         -157.6800, -121.9500, -207.9600, -161.6000,    0.0000,    0.0000,\n",
            "            0.0000,    0.0000],\n",
            "        [ -54.5400,  -42.6100,  -78.6700,  -60.5900, -114.3100,  -88.4400,\n",
            "         -158.3500, -122.7700, -207.1500, -161.3700,    0.0000,    0.0000,\n",
            "            0.0000,    0.0000],\n",
            "        [ -54.4700,  -41.8700,  -79.1500,  -60.5800, -115.7200,  -89.2600,\n",
            "         -159.2300, -122.9500, -200.9800, -157.1800,    0.0000,    0.0000,\n",
            "            0.0000,    0.0000]])\n",
            "tensor([[-11.9800],\n",
            "        [-12.1500],\n",
            "        [-12.5800],\n",
            "        [-13.7900],\n",
            "        [-15.7300],\n",
            "        [-58.4100],\n",
            "        [-59.6800],\n",
            "        [-60.1200],\n",
            "        [-60.4100],\n",
            "        [-60.8700],\n",
            "        [-59.4100],\n",
            "        [-61.1600],\n",
            "        [-61.8000],\n",
            "        [-61.9400],\n",
            "        [-62.3400],\n",
            "        [-44.1000],\n",
            "        [-44.6700],\n",
            "        [-44.5000],\n",
            "        [-44.3400],\n",
            "        [-44.8000],\n",
            "        [-52.2200],\n",
            "        [-54.1500],\n",
            "        [-54.7200],\n",
            "        [-54.5400],\n",
            "        [-54.4700]])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        }
      ],
      "source": [
        "#lectura de los datos del archivo csv \n",
        "xy = np.loadtxt('QCM10.csv', delimiter=\";\", dtype=np.float32, skiprows=1)\n",
        "X = xy[:,0:14]\n",
        "y = xy[:, [0]]\n",
        "\n",
        "#se dividen los datos en entrada(X) y salida esperada(y)\n",
        "X = torch.tensor(X, dtype = torch.float)\n",
        "y = torch.tensor(y, dtype = torch.float)\n",
        "\n",
        "#se genera una matriz de prediccion de los valores de entrada\n",
        "xPredicted = torch.tensor(X, dtype = torch.float)\n",
        "\n",
        "#se impriment las variables\n",
        "print(X)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normalización de los valore de entrada y salida**"
      ],
      "metadata": {
        "id": "l_aRQR_1zGX-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydI5RY00vHrx",
        "outputId": "9af0c2c6-43b7-420b-dbc8-ce6af227f7d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ -11.9800,  -10.9900,  -19.1200,  -17.2800,  -33.1300,  -28.4500,\n",
            "          -48.8300,  -40.7700,  -62.4900,  -50.8200,    1.0000,    0.0000,\n",
            "            0.0000,    0.0000],\n",
            "        [ -12.1500,  -11.3300,  -22.3300,  -19.9500,  -39.8200,  -33.6400,\n",
            "          -56.9000,  -46.7700,  -73.3200,  -58.9600,    1.0000,    0.0000,\n",
            "            0.0000,    0.0000],\n",
            "        [ -12.5800,  -11.7400,  -26.6700,  -23.3400,  -46.4800,  -38.6900,\n",
            "          -65.9500,  -53.4600,  -84.5300,  -67.2100,    1.0000,    0.0000,\n",
            "            0.0000,    0.0000],\n",
            "        [ -13.7900,  -12.8200,  -30.5600,  -26.1800,  -52.3000,  -42.9800,\n",
            "          -73.8100,  -59.1900,  -94.4100,  -74.4000,    1.0000,    0.0000,\n",
            "            0.0000,    0.0000],\n",
            "        [ -15.7300,  -13.8700,  -34.5400,  -28.6500,  -57.4400,  -46.2600,\n",
            "          -80.3700,  -63.4900, -102.9400,  -80.2500,    1.0000,    0.0000,\n",
            "            0.0000,    0.0000],\n",
            "        [ -58.4100,  -38.6600,  -83.5800,  -57.3300, -110.2400,  -76.8000,\n",
            "         -133.7700,  -96.1300, -171.0500, -124.1500,    0.0000,    1.0000,\n",
            "            0.0000,    0.0000],\n",
            "        [ -59.6800,  -40.0100,  -84.7500,  -58.7500, -109.9100,  -77.2800,\n",
            "         -130.6500,  -94.6900, -161.8100, -118.7000,    0.0000,    1.0000,\n",
            "            0.0000,    0.0000],\n",
            "        [ -60.1200,  -40.7300,  -85.1900,  -59.4700, -108.0800,  -76.4300,\n",
            "         -127.5100,  -92.8800, -155.5400, -114.7600,    0.0000,    1.0000,\n",
            "            0.0000,    0.0000],\n",
            "        [ -60.4100,  -41.2000,  -84.7300,  -59.3100, -106.4600,  -75.3900,\n",
            "         -124.6200,  -90.8800, -151.5600, -112.5800,    0.0000,    1.0000,\n",
            "            0.0000,    0.0000],\n",
            "        [ -60.8700,  -39.8800,  -84.2200,  -57.5800, -105.3200,  -73.3600,\n",
            "         -122.5900,  -88.2800, -146.9900, -109.5200,    0.0000,    1.0000,\n",
            "            0.0000,    0.0000],\n",
            "        [ -59.4100,  -61.8600,  -91.6500,  -81.0200, -131.0500, -105.5100,\n",
            "         -165.6800, -126.3000, -207.1600, -153.4400,    0.0000,    0.0000,\n",
            "            1.0000,    0.0000],\n",
            "        [ -61.1600,  -62.9200,  -93.5500,  -82.3900, -133.1100, -107.2700,\n",
            "         -168.6200, -128.9400, -210.8500, -157.0100,    0.0000,    0.0000,\n",
            "            1.0000,    0.0000],\n",
            "        [ -61.8000,  -62.9900,  -94.1500,  -82.4500, -133.2400, -107.1500,\n",
            "         -165.3300, -126.5500, -204.6000, -152.8100,    0.0000,    0.0000,\n",
            "            1.0000,    0.0000],\n",
            "        [ -61.9400,  -62.7800,  -94.4600,  -82.3900, -131.5400, -105.6000,\n",
            "         -160.7400, -123.0800, -195.4900, -146.2600,    0.0000,    0.0000,\n",
            "            1.0000,    0.0000],\n",
            "        [ -62.3400,  -61.8700,  -95.0900,  -81.5300, -128.2300, -102.0500,\n",
            "         -156.3100, -119.7100, -187.9500, -140.7900,    0.0000,    0.0000,\n",
            "            1.0000,    0.0000],\n",
            "        [ -44.1000,  -35.0200,  -66.1300,  -50.4300,  -90.2900,  -68.0000,\n",
            "         -101.7400,  -76.5300, -117.6700,  -88.6300,    0.0000,    0.0000,\n",
            "            0.0000,    1.0000],\n",
            "        [ -44.6700,  -35.3900,  -66.6200,  -50.8400,  -84.4100,  -63.9400,\n",
            "          -97.1100,  -73.3500, -111.0600,  -83.9800,    0.0000,    0.0000,\n",
            "            0.0000,    1.0000],\n",
            "        [ -44.5000,  -35.2700,  -66.6000,  -50.9800,  -80.7900,  -61.2400,\n",
            "          -93.3400,  -70.6700, -106.1800,  -80.4500,    0.0000,    0.0000,\n",
            "            0.0000,    1.0000],\n",
            "        [ -44.3400,  -34.9700,  -66.5800,  -50.9300,  -78.0700,  -59.2700,\n",
            "          -89.9500,  -68.1800, -101.4600,  -77.0600,    0.0000,    0.0000,\n",
            "            0.0000,    1.0000],\n",
            "        [ -44.8000,  -35.1700,  -66.2000,  -50.4200,  -76.8000,  -58.2600,\n",
            "          -87.0900,  -65.5500,  -98.1400,  -73.8900,    0.0000,    0.0000,\n",
            "            0.0000,    1.0000],\n",
            "        [ -52.2200,  -41.6700,  -75.7400,  -57.7000, -109.1000,  -83.4200,\n",
            "         -149.6200, -114.9600, -197.3500, -151.9200,    0.0000,    0.0000,\n",
            "            0.0000,    0.0000],\n",
            "        [ -54.1500,  -42.9100,  -77.4300,  -59.3300, -112.0400,  -86.1600,\n",
            "         -155.0400, -119.5600, -205.7700, -159.3000,    0.0000,    0.0000,\n",
            "            0.0000,    0.0000],\n",
            "        [ -54.7200,  -43.0400,  -78.1900,  -60.0500, -113.3600,  -87.5000,\n",
            "         -157.6800, -121.9500, -207.9600, -161.6000,    0.0000,    0.0000,\n",
            "            0.0000,    0.0000],\n",
            "        [ -54.5400,  -42.6100,  -78.6700,  -60.5900, -114.3100,  -88.4400,\n",
            "         -158.3500, -122.7700, -207.1500, -161.3700,    0.0000,    0.0000,\n",
            "            0.0000,    0.0000],\n",
            "        [ -54.4700,  -41.8700,  -79.1500,  -60.5800, -115.7200,  -89.2600,\n",
            "         -159.2300, -122.9500, -200.9800, -157.1800,    0.0000,    0.0000,\n",
            "            0.0000,    0.0000]])\n",
            "x  normalizado\n",
            "tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "         1.0000, 1.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [1.0142, 1.0309, 1.1679, 1.1545, 1.2019, 1.1824, 1.1653, 1.1472, 1.1733,\n",
            "         1.1602, 1.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [1.0501, 1.0682, 1.3949, 1.3507, 1.4030, 1.3599, 1.3506, 1.3113, 1.3527,\n",
            "         1.3225, 1.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [1.1511, 1.1665, 1.5983, 1.5150, 1.5786, 1.5107, 1.5116, 1.4518, 1.5108,\n",
            "         1.4640, 1.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [1.3130, 1.2621, 1.8065, 1.6580, 1.7338, 1.6260, 1.6459, 1.5573, 1.6473,\n",
            "         1.5791, 1.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [4.8756, 3.5177, 4.3713, 3.3177, 3.3275, 2.6995, 2.7395, 2.3579, 2.7372,\n",
            "         2.4429, 0.0000, 1.0000, 0.0000, 0.0000],\n",
            "        [4.9816, 3.6406, 4.4325, 3.3999, 3.3175, 2.7163, 2.6756, 2.3225, 2.5894,\n",
            "         2.3357, 0.0000, 1.0000, 0.0000, 0.0000],\n",
            "        [5.0184, 3.7061, 4.4555, 3.4416, 3.2623, 2.6865, 2.6113, 2.2781, 2.4890,\n",
            "         2.2582, 0.0000, 1.0000, 0.0000, 0.0000],\n",
            "        [5.0426, 3.7489, 4.4315, 3.4323, 3.2134, 2.6499, 2.5521, 2.2291, 2.4253,\n",
            "         2.2153, 0.0000, 1.0000, 0.0000, 0.0000],\n",
            "        [5.0810, 3.6288, 4.4048, 3.3322, 3.1790, 2.5786, 2.5105, 2.1653, 2.3522,\n",
            "         2.1551, 0.0000, 1.0000, 0.0000, 0.0000],\n",
            "        [4.9591, 5.6288, 4.7934, 4.6887, 3.9556, 3.7086, 3.3930, 3.0979, 3.3151,\n",
            "         3.0193, 0.0000, 0.0000, 1.0000, 0.0000],\n",
            "        [5.1052, 5.7252, 4.8928, 4.7679, 4.0178, 3.7705, 3.4532, 3.1626, 3.3741,\n",
            "         3.0895, 0.0000, 0.0000, 1.0000, 0.0000],\n",
            "        [5.1586, 5.7316, 4.9242, 4.7714, 4.0217, 3.7663, 3.3858, 3.1040, 3.2741,\n",
            "         3.0069, 0.0000, 0.0000, 1.0000, 0.0000],\n",
            "        [5.1703, 5.7125, 4.9404, 4.7679, 3.9704, 3.7118, 3.2918, 3.0189, 3.1283,\n",
            "         2.8780, 0.0000, 0.0000, 1.0000, 0.0000],\n",
            "        [5.2037, 5.6297, 4.9733, 4.7182, 3.8705, 3.5870, 3.2011, 2.9362, 3.0077,\n",
            "         2.7704, 0.0000, 0.0000, 1.0000, 0.0000],\n",
            "        [3.6811, 3.1865, 3.4587, 2.9184, 2.7253, 2.3902, 2.0836, 1.8771, 1.8830,\n",
            "         1.7440, 0.0000, 0.0000, 0.0000, 1.0000],\n",
            "        [3.7287, 3.2202, 3.4843, 2.9421, 2.5478, 2.2475, 1.9887, 1.7991, 1.7772,\n",
            "         1.6525, 0.0000, 0.0000, 0.0000, 1.0000],\n",
            "        [3.7145, 3.2093, 3.4833, 2.9502, 2.4386, 2.1525, 1.9115, 1.7334, 1.6992,\n",
            "         1.5830, 0.0000, 0.0000, 0.0000, 1.0000],\n",
            "        [3.7012, 3.1820, 3.4822, 2.9473, 2.3565, 2.0833, 1.8421, 1.6723, 1.6236,\n",
            "         1.5163, 0.0000, 0.0000, 0.0000, 1.0000],\n",
            "        [3.7396, 3.2002, 3.4623, 2.9178, 2.3181, 2.0478, 1.7835, 1.6078, 1.5705,\n",
            "         1.4540, 0.0000, 0.0000, 0.0000, 1.0000],\n",
            "        [4.3589, 3.7916, 3.9613, 3.3391, 3.2931, 2.9322, 3.0641, 2.8197, 3.1581,\n",
            "         2.9894, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [4.5200, 3.9045, 4.0497, 3.4334, 3.3818, 3.0285, 3.1751, 2.9325, 3.2928,\n",
            "         3.1346, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [4.5676, 3.9163, 4.0894, 3.4751, 3.4217, 3.0756, 3.2292, 2.9912, 3.3279,\n",
            "         3.1799, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [4.5526, 3.8772, 4.1145, 3.5064, 3.4503, 3.1086, 3.2429, 3.0113, 3.3149,\n",
            "         3.1753, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [4.5467, 3.8098, 4.1396, 3.5058, 3.4929, 3.1374, 3.2609, 3.0157, 3.2162,\n",
            "         3.0929, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
            "----dimensiones de la entrada-----\n",
            "torch.Size([25, 14])\n",
            "----dimensiones de la salida-----\n",
            "torch.Size([25, 1])\n"
          ]
        }
      ],
      "source": [
        "# calculo del valor maximo de X\n",
        "X_max, _ = torch.max(X, 0)\n",
        "# calculo del valor maximo de la entrada predecida\n",
        "xPredicted_max, _ = torch.max(xPredicted, 0)\n",
        "# normalizacion de los valores de entrada\n",
        "X = torch.div(X, X_max)\n",
        "# Normalizacion de los valores de entrada predecidos\n",
        "xPredicted_max = torch.div(xPredicted, xPredicted_max)\n",
        "# Normalizacion de los valores de salida\n",
        "y = y / 100\n",
        "\n",
        "print(xPredicted)\n",
        "print(\"x  normalizado\")\n",
        "print(X)\n",
        "print(\"----dimensiones de la entrada-----\")\n",
        "print(X.shape)\n",
        "\n",
        "print(\"----dimensiones de la salida-----\")\n",
        "print(y.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZ7S8q9_vHry"
      },
      "source": [
        "# Modelo de red neuronal de retropropagación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "CpFVDESkvHrz"
      },
      "outputs": [],
      "source": [
        "class Neural_Network(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super(Neural_Network, self).__init__()\n",
        "        # parametros(capas de la red).\n",
        "        self.inputSize = 14\n",
        "        self.outputSize = 1\n",
        "        self.hiddenSize = 25\n",
        "        \n",
        "        # pesos\n",
        "        self.W1 = torch.randn(self.inputSize, self.hiddenSize) # tensor de 14 X 25\n",
        "        self.W2 = torch.randn(self.hiddenSize, self.outputSize) # tensor de 25 X 1 \n",
        "        print(\"------pesos w1 --------------\")\n",
        "        print(self.W1.shape)\n",
        "        print(\"------pesos w1 --------------\")\n",
        "        print(self.W2.shape)\n",
        "\n",
        "    #fordward propagation\n",
        "    def forward(self, X):\n",
        "        # la primera salida sera el resultado de la multiplicacion matricial entre la entrada y los pesos\n",
        "        self.z = torch.matmul(X, self.W1) # 3 X 3 \".dot\" does not broadcast in PyTorch\n",
        "        # la salida sera alterada por la funcion de activacion sigmoidal\n",
        "        self.z2 = self.sigmoid(self.z) # activation function\n",
        "        # la tercera salida sera la multiplicacion matricial de la salida anterior por los nuevos pesos\n",
        "        self.z3 = torch.matmul(self.z2, self.W2)\n",
        "        # la ultima salida (el valor predecido) sera alterada por la funcion de activacion sigmoidal\n",
        "        o = self.sigmoid(self.z3) \n",
        "        return o\n",
        "\n",
        "    #calculo de la funcion sigmoidal de fordward propagation\n",
        "    def sigmoid(self, s):\n",
        "        return 1 / (1 + torch.exp(-s))\n",
        "    \n",
        "    #calculo de la derivada de la funcion sigmoidal, para la retropropagacion.\n",
        "    def sigmoidPrime(self, s):\n",
        "        return s * (1 - s)\n",
        "    \n",
        "    #retropropagacion\n",
        "    def backward(self, X, y, o):\n",
        "        # el error es la diferencia del valor obtenido y el esperado.\n",
        "        self.o_error = y - o \n",
        "        # calculo de la influencia de la salida en el error al variar el peso.\n",
        "        self.o_delta = self.o_error * self.sigmoidPrime(o)\n",
        "        # calculo de la influencia de la salida en el error al variar el peso.\n",
        "        self.z2_error = torch.matmul(self.o_delta, torch.t(self.W2))\n",
        "\n",
        "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.z2)\n",
        "        #re ajuste de los ultimos pesos\n",
        "        self.W2 += torch.matmul(torch.t(self.z2), self.o_delta)\n",
        "        #re ajuste de los pesos anteriores\n",
        "        self.W1 += torch.matmul(torch.t(X), self.z2_delta)\n",
        "\n",
        "        \n",
        "    # entrenamiendo de la red neuronal    \n",
        "    def train(self, X, y):\n",
        "        # fordward propagation\n",
        "        o = self.forward(X)\n",
        "        # backpropagation\n",
        "        self.backward(X, y, o)\n",
        "        \n",
        "    def saveWeights(self, model):\n",
        "        # guardado de los pesos con la funcion save de torch\n",
        "        torch.save(model, \"bpnn_saved\")\n",
        "        # el metodo de carga de pesos sera load(ruta_archivo)\n",
        "        \n",
        "    # prediccion de los valores, dado una matriz de mismas dimensionalidades.    \n",
        "    def predict(self):\n",
        "        print (\"Predicted data based on trained weights: \")\n",
        "        print (\"Input (scaled): \\n\" + str(xPredicted))\n",
        "        print (\"Output: \\n\" + str(self.forward(xPredicted)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AhS3ENavHr0",
        "outputId": "f6351dee-eaae-437b-905f-c11f0031b704"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------pesos w1 --------------\n",
            "torch.Size([14, 25])\n",
            "------pesos w1 --------------\n",
            "torch.Size([25, 1])\n",
            "#0 Loss: 1.9119738340377808\n",
            "#1 Loss: 0.24852041900157928\n",
            "#2 Loss: 0.24852041900157928\n",
            "#3 Loss: 0.24852041900157928\n",
            "#4 Loss: 0.24852041900157928\n",
            "#5 Loss: 0.24852041900157928\n",
            "#6 Loss: 0.24852041900157928\n",
            "#7 Loss: 0.24852041900157928\n",
            "#8 Loss: 0.24852041900157928\n",
            "#9 Loss: 0.24852041900157928\n",
            "#10 Loss: 0.24852041900157928\n",
            "#11 Loss: 0.24852041900157928\n",
            "#12 Loss: 0.24852041900157928\n",
            "#13 Loss: 0.24852041900157928\n",
            "#14 Loss: 0.24852041900157928\n",
            "#15 Loss: 0.24852041900157928\n",
            "#16 Loss: 0.24852041900157928\n",
            "#17 Loss: 0.24852041900157928\n",
            "#18 Loss: 0.24852041900157928\n",
            "#19 Loss: 0.24852041900157928\n",
            "#20 Loss: 0.24852041900157928\n",
            "#21 Loss: 0.24852041900157928\n",
            "#22 Loss: 0.24852041900157928\n",
            "#23 Loss: 0.24852041900157928\n",
            "#24 Loss: 0.24852041900157928\n",
            "#25 Loss: 0.24852041900157928\n",
            "#26 Loss: 0.24852041900157928\n",
            "#27 Loss: 0.24852041900157928\n",
            "#28 Loss: 0.24852041900157928\n",
            "#29 Loss: 0.24852041900157928\n",
            "#30 Loss: 0.24852041900157928\n",
            "#31 Loss: 0.24852041900157928\n",
            "#32 Loss: 0.24852041900157928\n",
            "#33 Loss: 0.24852041900157928\n",
            "#34 Loss: 0.24852041900157928\n",
            "#35 Loss: 0.24852041900157928\n",
            "#36 Loss: 0.24852041900157928\n",
            "#37 Loss: 0.24852041900157928\n",
            "#38 Loss: 0.24852041900157928\n",
            "#39 Loss: 0.24852041900157928\n",
            "#40 Loss: 0.24852041900157928\n",
            "#41 Loss: 0.24852041900157928\n",
            "#42 Loss: 0.24852041900157928\n",
            "#43 Loss: 0.24852041900157928\n",
            "#44 Loss: 0.24852041900157928\n",
            "#45 Loss: 0.24852041900157928\n",
            "#46 Loss: 0.24852041900157928\n",
            "#47 Loss: 0.24852041900157928\n",
            "#48 Loss: 0.24852041900157928\n",
            "#49 Loss: 0.24852041900157928\n",
            "#50 Loss: 0.24852041900157928\n",
            "#51 Loss: 0.24852041900157928\n",
            "#52 Loss: 0.24852041900157928\n",
            "#53 Loss: 0.24852041900157928\n",
            "#54 Loss: 0.24852041900157928\n",
            "#55 Loss: 0.24852041900157928\n",
            "#56 Loss: 0.24852041900157928\n",
            "#57 Loss: 0.24852041900157928\n",
            "#58 Loss: 0.24852041900157928\n",
            "#59 Loss: 0.24852041900157928\n",
            "#60 Loss: 0.24852041900157928\n",
            "#61 Loss: 0.24852041900157928\n",
            "#62 Loss: 0.24852041900157928\n",
            "#63 Loss: 0.24852041900157928\n",
            "#64 Loss: 0.24852041900157928\n",
            "#65 Loss: 0.24852041900157928\n",
            "#66 Loss: 0.24852041900157928\n",
            "#67 Loss: 0.24852041900157928\n",
            "#68 Loss: 0.24852041900157928\n",
            "#69 Loss: 0.24852041900157928\n",
            "#70 Loss: 0.24852041900157928\n",
            "#71 Loss: 0.24852041900157928\n",
            "#72 Loss: 0.24852041900157928\n",
            "#73 Loss: 0.24852041900157928\n",
            "#74 Loss: 0.24852041900157928\n",
            "#75 Loss: 0.24852041900157928\n",
            "#76 Loss: 0.24852041900157928\n",
            "#77 Loss: 0.24852041900157928\n",
            "#78 Loss: 0.24852041900157928\n",
            "#79 Loss: 0.24852041900157928\n",
            "#80 Loss: 0.24852041900157928\n",
            "#81 Loss: 0.24852041900157928\n",
            "#82 Loss: 0.24852041900157928\n",
            "#83 Loss: 0.24852041900157928\n",
            "#84 Loss: 0.24852041900157928\n",
            "#85 Loss: 0.24852041900157928\n",
            "#86 Loss: 0.24852041900157928\n",
            "#87 Loss: 0.24852041900157928\n",
            "#88 Loss: 0.24852041900157928\n",
            "#89 Loss: 0.24852041900157928\n",
            "#90 Loss: 0.24852041900157928\n",
            "#91 Loss: 0.24852041900157928\n",
            "#92 Loss: 0.24852041900157928\n",
            "#93 Loss: 0.24852041900157928\n",
            "#94 Loss: 0.24852041900157928\n",
            "#95 Loss: 0.24852041900157928\n",
            "#96 Loss: 0.24852041900157928\n",
            "#97 Loss: 0.24852041900157928\n",
            "#98 Loss: 0.24852041900157928\n",
            "#99 Loss: 0.24852041900157928\n",
            "#100 Loss: 0.24852041900157928\n",
            "#101 Loss: 0.24852041900157928\n",
            "#102 Loss: 0.24852041900157928\n",
            "#103 Loss: 0.24852041900157928\n",
            "#104 Loss: 0.24852041900157928\n",
            "#105 Loss: 0.24852041900157928\n",
            "#106 Loss: 0.24852041900157928\n",
            "#107 Loss: 0.24852041900157928\n",
            "#108 Loss: 0.24852041900157928\n",
            "#109 Loss: 0.24852041900157928\n",
            "#110 Loss: 0.24852041900157928\n",
            "#111 Loss: 0.24852041900157928\n",
            "#112 Loss: 0.24852041900157928\n",
            "#113 Loss: 0.24852041900157928\n",
            "#114 Loss: 0.24852041900157928\n",
            "#115 Loss: 0.24852041900157928\n",
            "#116 Loss: 0.24852041900157928\n",
            "#117 Loss: 0.24852041900157928\n",
            "#118 Loss: 0.24852041900157928\n",
            "#119 Loss: 0.24852041900157928\n",
            "#120 Loss: 0.24852041900157928\n",
            "#121 Loss: 0.24852041900157928\n",
            "#122 Loss: 0.24852041900157928\n",
            "#123 Loss: 0.24852041900157928\n",
            "#124 Loss: 0.24852041900157928\n",
            "#125 Loss: 0.24852041900157928\n",
            "#126 Loss: 0.24852041900157928\n",
            "#127 Loss: 0.24852041900157928\n",
            "#128 Loss: 0.24852041900157928\n",
            "#129 Loss: 0.24852041900157928\n",
            "#130 Loss: 0.24852041900157928\n",
            "#131 Loss: 0.24852041900157928\n",
            "#132 Loss: 0.24852041900157928\n",
            "#133 Loss: 0.24852041900157928\n",
            "#134 Loss: 0.24852041900157928\n",
            "#135 Loss: 0.24852041900157928\n",
            "#136 Loss: 0.24852041900157928\n",
            "#137 Loss: 0.24852041900157928\n",
            "#138 Loss: 0.24852041900157928\n",
            "#139 Loss: 0.24852041900157928\n",
            "#140 Loss: 0.24852041900157928\n",
            "#141 Loss: 0.24852041900157928\n",
            "#142 Loss: 0.24852041900157928\n",
            "#143 Loss: 0.24852041900157928\n",
            "#144 Loss: 0.24852041900157928\n",
            "#145 Loss: 0.24852041900157928\n",
            "#146 Loss: 0.24852041900157928\n",
            "#147 Loss: 0.24852041900157928\n",
            "#148 Loss: 0.24852041900157928\n",
            "#149 Loss: 0.24852041900157928\n",
            "#150 Loss: 0.24852041900157928\n",
            "#151 Loss: 0.24852041900157928\n",
            "#152 Loss: 0.24852041900157928\n",
            "#153 Loss: 0.24852041900157928\n",
            "#154 Loss: 0.24852041900157928\n",
            "#155 Loss: 0.24852041900157928\n",
            "#156 Loss: 0.24852041900157928\n",
            "#157 Loss: 0.24852041900157928\n",
            "#158 Loss: 0.24852041900157928\n",
            "#159 Loss: 0.24852041900157928\n",
            "#160 Loss: 0.24852041900157928\n",
            "#161 Loss: 0.24852041900157928\n",
            "#162 Loss: 0.24852041900157928\n",
            "#163 Loss: 0.24852041900157928\n",
            "#164 Loss: 0.24852041900157928\n",
            "#165 Loss: 0.24852041900157928\n",
            "#166 Loss: 0.24852041900157928\n",
            "#167 Loss: 0.24852041900157928\n",
            "#168 Loss: 0.24852041900157928\n",
            "#169 Loss: 0.24852041900157928\n",
            "#170 Loss: 0.24852041900157928\n",
            "#171 Loss: 0.24852041900157928\n",
            "#172 Loss: 0.24852041900157928\n",
            "#173 Loss: 0.24852041900157928\n",
            "#174 Loss: 0.24852041900157928\n",
            "#175 Loss: 0.24852041900157928\n",
            "#176 Loss: 0.24852041900157928\n",
            "#177 Loss: 0.24852041900157928\n",
            "#178 Loss: 0.24852041900157928\n",
            "#179 Loss: 0.24852041900157928\n",
            "#180 Loss: 0.24852041900157928\n",
            "#181 Loss: 0.24852041900157928\n",
            "#182 Loss: 0.24852041900157928\n",
            "#183 Loss: 0.24852041900157928\n",
            "#184 Loss: 0.24852041900157928\n",
            "#185 Loss: 0.24852041900157928\n",
            "#186 Loss: 0.24852041900157928\n",
            "#187 Loss: 0.24852041900157928\n",
            "#188 Loss: 0.24852041900157928\n",
            "#189 Loss: 0.24852041900157928\n",
            "#190 Loss: 0.24852041900157928\n",
            "#191 Loss: 0.24852041900157928\n",
            "#192 Loss: 0.24852041900157928\n",
            "#193 Loss: 0.24852041900157928\n",
            "#194 Loss: 0.24852041900157928\n",
            "#195 Loss: 0.24852041900157928\n",
            "#196 Loss: 0.24852041900157928\n",
            "#197 Loss: 0.24852041900157928\n",
            "#198 Loss: 0.24852041900157928\n",
            "#199 Loss: 0.24852041900157928\n",
            "#200 Loss: 0.24852041900157928\n",
            "#201 Loss: 0.24852041900157928\n",
            "#202 Loss: 0.24852041900157928\n",
            "#203 Loss: 0.24852041900157928\n",
            "#204 Loss: 0.24852041900157928\n",
            "#205 Loss: 0.24852041900157928\n",
            "#206 Loss: 0.24852041900157928\n",
            "#207 Loss: 0.24852041900157928\n",
            "#208 Loss: 0.24852041900157928\n",
            "#209 Loss: 0.24852041900157928\n",
            "#210 Loss: 0.24852041900157928\n",
            "#211 Loss: 0.24852041900157928\n",
            "#212 Loss: 0.24852041900157928\n",
            "#213 Loss: 0.24852041900157928\n",
            "#214 Loss: 0.24852041900157928\n",
            "#215 Loss: 0.24852041900157928\n",
            "#216 Loss: 0.24852041900157928\n",
            "#217 Loss: 0.24852041900157928\n",
            "#218 Loss: 0.24852041900157928\n",
            "#219 Loss: 0.24852041900157928\n",
            "#220 Loss: 0.24852041900157928\n",
            "#221 Loss: 0.24852041900157928\n",
            "#222 Loss: 0.24852041900157928\n",
            "#223 Loss: 0.24852041900157928\n",
            "#224 Loss: 0.24852041900157928\n",
            "#225 Loss: 0.24852041900157928\n",
            "#226 Loss: 0.24852041900157928\n",
            "#227 Loss: 0.24852041900157928\n",
            "#228 Loss: 0.24852041900157928\n",
            "#229 Loss: 0.24852041900157928\n",
            "#230 Loss: 0.24852041900157928\n",
            "#231 Loss: 0.24852041900157928\n",
            "#232 Loss: 0.24852041900157928\n",
            "#233 Loss: 0.24852041900157928\n",
            "#234 Loss: 0.24852041900157928\n",
            "#235 Loss: 0.24852041900157928\n",
            "#236 Loss: 0.24852041900157928\n",
            "#237 Loss: 0.24852041900157928\n",
            "#238 Loss: 0.24852041900157928\n",
            "#239 Loss: 0.24852041900157928\n",
            "#240 Loss: 0.24852041900157928\n",
            "#241 Loss: 0.24852041900157928\n",
            "#242 Loss: 0.24852041900157928\n",
            "#243 Loss: 0.24852041900157928\n",
            "#244 Loss: 0.24852041900157928\n",
            "#245 Loss: 0.24852041900157928\n",
            "#246 Loss: 0.24852041900157928\n",
            "#247 Loss: 0.24852041900157928\n",
            "#248 Loss: 0.24852041900157928\n",
            "#249 Loss: 0.24852041900157928\n",
            "#250 Loss: 0.24852041900157928\n",
            "#251 Loss: 0.24852041900157928\n",
            "#252 Loss: 0.24852041900157928\n",
            "#253 Loss: 0.24852041900157928\n",
            "#254 Loss: 0.24852041900157928\n",
            "#255 Loss: 0.24852041900157928\n",
            "#256 Loss: 0.24852041900157928\n",
            "#257 Loss: 0.24852041900157928\n",
            "#258 Loss: 0.24852041900157928\n",
            "#259 Loss: 0.24852041900157928\n",
            "#260 Loss: 0.24852041900157928\n",
            "#261 Loss: 0.24852041900157928\n",
            "#262 Loss: 0.24852041900157928\n",
            "#263 Loss: 0.24852041900157928\n",
            "#264 Loss: 0.24852041900157928\n",
            "#265 Loss: 0.24852041900157928\n",
            "#266 Loss: 0.24852041900157928\n",
            "#267 Loss: 0.24852041900157928\n",
            "#268 Loss: 0.24852041900157928\n",
            "#269 Loss: 0.24852041900157928\n",
            "#270 Loss: 0.24852041900157928\n",
            "#271 Loss: 0.24852041900157928\n",
            "#272 Loss: 0.24852041900157928\n",
            "#273 Loss: 0.24852041900157928\n",
            "#274 Loss: 0.24852041900157928\n",
            "#275 Loss: 0.24852041900157928\n",
            "#276 Loss: 0.24852041900157928\n",
            "#277 Loss: 0.24852041900157928\n",
            "#278 Loss: 0.24852041900157928\n",
            "#279 Loss: 0.24852041900157928\n",
            "#280 Loss: 0.24852041900157928\n",
            "#281 Loss: 0.24852041900157928\n",
            "#282 Loss: 0.24852041900157928\n",
            "#283 Loss: 0.24852041900157928\n",
            "#284 Loss: 0.24852041900157928\n",
            "#285 Loss: 0.24852041900157928\n",
            "#286 Loss: 0.24852041900157928\n",
            "#287 Loss: 0.24852041900157928\n",
            "#288 Loss: 0.24852041900157928\n",
            "#289 Loss: 0.24852041900157928\n",
            "#290 Loss: 0.24852041900157928\n",
            "#291 Loss: 0.24852041900157928\n",
            "#292 Loss: 0.24852041900157928\n",
            "#293 Loss: 0.24852041900157928\n",
            "#294 Loss: 0.24852041900157928\n",
            "#295 Loss: 0.24852041900157928\n",
            "#296 Loss: 0.24852041900157928\n",
            "#297 Loss: 0.24852041900157928\n",
            "#298 Loss: 0.24852041900157928\n",
            "#299 Loss: 0.24852041900157928\n",
            "#300 Loss: 0.24852041900157928\n",
            "#301 Loss: 0.24852041900157928\n",
            "#302 Loss: 0.24852041900157928\n",
            "#303 Loss: 0.24852041900157928\n",
            "#304 Loss: 0.24852041900157928\n",
            "#305 Loss: 0.24852041900157928\n",
            "#306 Loss: 0.24852041900157928\n",
            "#307 Loss: 0.24852041900157928\n",
            "#308 Loss: 0.24852041900157928\n",
            "#309 Loss: 0.24852041900157928\n",
            "#310 Loss: 0.24852041900157928\n",
            "#311 Loss: 0.24852041900157928\n",
            "#312 Loss: 0.24852041900157928\n",
            "#313 Loss: 0.24852041900157928\n",
            "#314 Loss: 0.24852041900157928\n",
            "#315 Loss: 0.24852041900157928\n",
            "#316 Loss: 0.24852041900157928\n",
            "#317 Loss: 0.24852041900157928\n",
            "#318 Loss: 0.24852041900157928\n",
            "#319 Loss: 0.24852041900157928\n",
            "#320 Loss: 0.24852041900157928\n",
            "#321 Loss: 0.24852041900157928\n",
            "#322 Loss: 0.24852041900157928\n",
            "#323 Loss: 0.24852041900157928\n",
            "#324 Loss: 0.24852041900157928\n",
            "#325 Loss: 0.24852041900157928\n",
            "#326 Loss: 0.24852041900157928\n",
            "#327 Loss: 0.24852041900157928\n",
            "#328 Loss: 0.24852041900157928\n",
            "#329 Loss: 0.24852041900157928\n",
            "#330 Loss: 0.24852041900157928\n",
            "#331 Loss: 0.24852041900157928\n",
            "#332 Loss: 0.24852041900157928\n",
            "#333 Loss: 0.24852041900157928\n",
            "#334 Loss: 0.24852041900157928\n",
            "#335 Loss: 0.24852041900157928\n",
            "#336 Loss: 0.24852041900157928\n",
            "#337 Loss: 0.24852041900157928\n",
            "#338 Loss: 0.24852041900157928\n",
            "#339 Loss: 0.24852041900157928\n",
            "#340 Loss: 0.24852041900157928\n",
            "#341 Loss: 0.24852041900157928\n",
            "#342 Loss: 0.24852041900157928\n",
            "#343 Loss: 0.24852041900157928\n",
            "#344 Loss: 0.24852041900157928\n",
            "#345 Loss: 0.24852041900157928\n",
            "#346 Loss: 0.24852041900157928\n",
            "#347 Loss: 0.24852041900157928\n",
            "#348 Loss: 0.24852041900157928\n",
            "#349 Loss: 0.24852041900157928\n",
            "#350 Loss: 0.24852041900157928\n",
            "#351 Loss: 0.24852041900157928\n",
            "#352 Loss: 0.24852041900157928\n",
            "#353 Loss: 0.24852041900157928\n",
            "#354 Loss: 0.24852041900157928\n",
            "#355 Loss: 0.24852041900157928\n",
            "#356 Loss: 0.24852041900157928\n",
            "#357 Loss: 0.24852041900157928\n",
            "#358 Loss: 0.24852041900157928\n",
            "#359 Loss: 0.24852041900157928\n",
            "#360 Loss: 0.24852041900157928\n",
            "#361 Loss: 0.24852041900157928\n",
            "#362 Loss: 0.24852041900157928\n",
            "#363 Loss: 0.24852041900157928\n",
            "#364 Loss: 0.24852041900157928\n",
            "#365 Loss: 0.24852041900157928\n",
            "#366 Loss: 0.24852041900157928\n",
            "#367 Loss: 0.24852041900157928\n",
            "#368 Loss: 0.24852041900157928\n",
            "#369 Loss: 0.24852041900157928\n",
            "#370 Loss: 0.24852041900157928\n",
            "#371 Loss: 0.24852041900157928\n",
            "#372 Loss: 0.24852041900157928\n",
            "#373 Loss: 0.24852041900157928\n",
            "#374 Loss: 0.24852041900157928\n",
            "#375 Loss: 0.24852041900157928\n",
            "#376 Loss: 0.24852041900157928\n",
            "#377 Loss: 0.24852041900157928\n",
            "#378 Loss: 0.24852041900157928\n",
            "#379 Loss: 0.24852041900157928\n",
            "#380 Loss: 0.24852041900157928\n",
            "#381 Loss: 0.24852041900157928\n",
            "#382 Loss: 0.24852041900157928\n",
            "#383 Loss: 0.24852041900157928\n",
            "#384 Loss: 0.24852041900157928\n",
            "#385 Loss: 0.24852041900157928\n",
            "#386 Loss: 0.24852041900157928\n",
            "#387 Loss: 0.24852041900157928\n",
            "#388 Loss: 0.24852041900157928\n",
            "#389 Loss: 0.24852041900157928\n",
            "#390 Loss: 0.24852041900157928\n",
            "#391 Loss: 0.24852041900157928\n",
            "#392 Loss: 0.24852041900157928\n",
            "#393 Loss: 0.24852041900157928\n",
            "#394 Loss: 0.24852041900157928\n",
            "#395 Loss: 0.24852041900157928\n",
            "#396 Loss: 0.24852041900157928\n",
            "#397 Loss: 0.24852041900157928\n",
            "#398 Loss: 0.24852041900157928\n",
            "#399 Loss: 0.24852041900157928\n",
            "#400 Loss: 0.24852041900157928\n",
            "#401 Loss: 0.24852041900157928\n",
            "#402 Loss: 0.24852041900157928\n",
            "#403 Loss: 0.24852041900157928\n",
            "#404 Loss: 0.24852041900157928\n",
            "#405 Loss: 0.24852041900157928\n",
            "#406 Loss: 0.24852041900157928\n",
            "#407 Loss: 0.24852041900157928\n",
            "#408 Loss: 0.24852041900157928\n",
            "#409 Loss: 0.24852041900157928\n",
            "#410 Loss: 0.24852041900157928\n",
            "#411 Loss: 0.24852041900157928\n",
            "#412 Loss: 0.24852041900157928\n",
            "#413 Loss: 0.24852041900157928\n",
            "#414 Loss: 0.24852041900157928\n",
            "#415 Loss: 0.24852041900157928\n",
            "#416 Loss: 0.24852041900157928\n",
            "#417 Loss: 0.24852041900157928\n",
            "#418 Loss: 0.24852041900157928\n",
            "#419 Loss: 0.24852041900157928\n",
            "#420 Loss: 0.24852041900157928\n",
            "#421 Loss: 0.24852041900157928\n",
            "#422 Loss: 0.24852041900157928\n",
            "#423 Loss: 0.24852041900157928\n",
            "#424 Loss: 0.24852041900157928\n",
            "#425 Loss: 0.24852041900157928\n",
            "#426 Loss: 0.24852041900157928\n",
            "#427 Loss: 0.24852041900157928\n",
            "#428 Loss: 0.24852041900157928\n",
            "#429 Loss: 0.24852041900157928\n",
            "#430 Loss: 0.24852041900157928\n",
            "#431 Loss: 0.24852041900157928\n",
            "#432 Loss: 0.24852041900157928\n",
            "#433 Loss: 0.24852041900157928\n",
            "#434 Loss: 0.24852041900157928\n",
            "#435 Loss: 0.24852041900157928\n",
            "#436 Loss: 0.24852041900157928\n",
            "#437 Loss: 0.24852041900157928\n",
            "#438 Loss: 0.24852041900157928\n",
            "#439 Loss: 0.24852041900157928\n",
            "#440 Loss: 0.24852041900157928\n",
            "#441 Loss: 0.24852041900157928\n",
            "#442 Loss: 0.24852041900157928\n",
            "#443 Loss: 0.24852041900157928\n",
            "#444 Loss: 0.24852041900157928\n",
            "#445 Loss: 0.24852041900157928\n",
            "#446 Loss: 0.24852041900157928\n",
            "#447 Loss: 0.24852041900157928\n",
            "#448 Loss: 0.24852041900157928\n",
            "#449 Loss: 0.24852041900157928\n",
            "#450 Loss: 0.24852041900157928\n",
            "#451 Loss: 0.24852041900157928\n",
            "#452 Loss: 0.24852041900157928\n",
            "#453 Loss: 0.24852041900157928\n",
            "#454 Loss: 0.24852041900157928\n",
            "#455 Loss: 0.24852041900157928\n",
            "#456 Loss: 0.24852041900157928\n",
            "#457 Loss: 0.24852041900157928\n",
            "#458 Loss: 0.24852041900157928\n",
            "#459 Loss: 0.24852041900157928\n",
            "#460 Loss: 0.24852041900157928\n",
            "#461 Loss: 0.24852041900157928\n",
            "#462 Loss: 0.24852041900157928\n",
            "#463 Loss: 0.24852041900157928\n",
            "#464 Loss: 0.24852041900157928\n",
            "#465 Loss: 0.24852041900157928\n",
            "#466 Loss: 0.24852041900157928\n",
            "#467 Loss: 0.24852041900157928\n",
            "#468 Loss: 0.24852041900157928\n",
            "#469 Loss: 0.24852041900157928\n",
            "#470 Loss: 0.24852041900157928\n",
            "#471 Loss: 0.24852041900157928\n",
            "#472 Loss: 0.24852041900157928\n",
            "#473 Loss: 0.24852041900157928\n",
            "#474 Loss: 0.24852041900157928\n",
            "#475 Loss: 0.24852041900157928\n",
            "#476 Loss: 0.24852041900157928\n",
            "#477 Loss: 0.24852041900157928\n",
            "#478 Loss: 0.24852041900157928\n",
            "#479 Loss: 0.24852041900157928\n",
            "#480 Loss: 0.24852041900157928\n",
            "#481 Loss: 0.24852041900157928\n",
            "#482 Loss: 0.24852041900157928\n",
            "#483 Loss: 0.24852041900157928\n",
            "#484 Loss: 0.24852041900157928\n",
            "#485 Loss: 0.24852041900157928\n",
            "#486 Loss: 0.24852041900157928\n",
            "#487 Loss: 0.24852041900157928\n",
            "#488 Loss: 0.24852041900157928\n",
            "#489 Loss: 0.24852041900157928\n",
            "#490 Loss: 0.24852041900157928\n",
            "#491 Loss: 0.24852041900157928\n",
            "#492 Loss: 0.24852041900157928\n",
            "#493 Loss: 0.24852041900157928\n",
            "#494 Loss: 0.24852041900157928\n",
            "#495 Loss: 0.24852041900157928\n",
            "#496 Loss: 0.24852041900157928\n",
            "#497 Loss: 0.24852041900157928\n",
            "#498 Loss: 0.24852041900157928\n",
            "#499 Loss: 0.24852041900157928\n",
            "Predicted data based on trained weights: \n",
            "Input (scaled): \n",
            "tensor([[ -11.9800,  -10.9900,  -19.1200,  -17.2800,  -33.1300,  -28.4500,\n",
            "          -48.8300,  -40.7700,  -62.4900,  -50.8200,    1.0000,    0.0000,\n",
            "            0.0000,    0.0000],\n",
            "        [ -12.1500,  -11.3300,  -22.3300,  -19.9500,  -39.8200,  -33.6400,\n",
            "          -56.9000,  -46.7700,  -73.3200,  -58.9600,    1.0000,    0.0000,\n",
            "            0.0000,    0.0000],\n",
            "        [ -12.5800,  -11.7400,  -26.6700,  -23.3400,  -46.4800,  -38.6900,\n",
            "          -65.9500,  -53.4600,  -84.5300,  -67.2100,    1.0000,    0.0000,\n",
            "            0.0000,    0.0000],\n",
            "        [ -13.7900,  -12.8200,  -30.5600,  -26.1800,  -52.3000,  -42.9800,\n",
            "          -73.8100,  -59.1900,  -94.4100,  -74.4000,    1.0000,    0.0000,\n",
            "            0.0000,    0.0000],\n",
            "        [ -15.7300,  -13.8700,  -34.5400,  -28.6500,  -57.4400,  -46.2600,\n",
            "          -80.3700,  -63.4900, -102.9400,  -80.2500,    1.0000,    0.0000,\n",
            "            0.0000,    0.0000],\n",
            "        [ -58.4100,  -38.6600,  -83.5800,  -57.3300, -110.2400,  -76.8000,\n",
            "         -133.7700,  -96.1300, -171.0500, -124.1500,    0.0000,    1.0000,\n",
            "            0.0000,    0.0000],\n",
            "        [ -59.6800,  -40.0100,  -84.7500,  -58.7500, -109.9100,  -77.2800,\n",
            "         -130.6500,  -94.6900, -161.8100, -118.7000,    0.0000,    1.0000,\n",
            "            0.0000,    0.0000],\n",
            "        [ -60.1200,  -40.7300,  -85.1900,  -59.4700, -108.0800,  -76.4300,\n",
            "         -127.5100,  -92.8800, -155.5400, -114.7600,    0.0000,    1.0000,\n",
            "            0.0000,    0.0000],\n",
            "        [ -60.4100,  -41.2000,  -84.7300,  -59.3100, -106.4600,  -75.3900,\n",
            "         -124.6200,  -90.8800, -151.5600, -112.5800,    0.0000,    1.0000,\n",
            "            0.0000,    0.0000],\n",
            "        [ -60.8700,  -39.8800,  -84.2200,  -57.5800, -105.3200,  -73.3600,\n",
            "         -122.5900,  -88.2800, -146.9900, -109.5200,    0.0000,    1.0000,\n",
            "            0.0000,    0.0000],\n",
            "        [ -59.4100,  -61.8600,  -91.6500,  -81.0200, -131.0500, -105.5100,\n",
            "         -165.6800, -126.3000, -207.1600, -153.4400,    0.0000,    0.0000,\n",
            "            1.0000,    0.0000],\n",
            "        [ -61.1600,  -62.9200,  -93.5500,  -82.3900, -133.1100, -107.2700,\n",
            "         -168.6200, -128.9400, -210.8500, -157.0100,    0.0000,    0.0000,\n",
            "            1.0000,    0.0000],\n",
            "        [ -61.8000,  -62.9900,  -94.1500,  -82.4500, -133.2400, -107.1500,\n",
            "         -165.3300, -126.5500, -204.6000, -152.8100,    0.0000,    0.0000,\n",
            "            1.0000,    0.0000],\n",
            "        [ -61.9400,  -62.7800,  -94.4600,  -82.3900, -131.5400, -105.6000,\n",
            "         -160.7400, -123.0800, -195.4900, -146.2600,    0.0000,    0.0000,\n",
            "            1.0000,    0.0000],\n",
            "        [ -62.3400,  -61.8700,  -95.0900,  -81.5300, -128.2300, -102.0500,\n",
            "         -156.3100, -119.7100, -187.9500, -140.7900,    0.0000,    0.0000,\n",
            "            1.0000,    0.0000],\n",
            "        [ -44.1000,  -35.0200,  -66.1300,  -50.4300,  -90.2900,  -68.0000,\n",
            "         -101.7400,  -76.5300, -117.6700,  -88.6300,    0.0000,    0.0000,\n",
            "            0.0000,    1.0000],\n",
            "        [ -44.6700,  -35.3900,  -66.6200,  -50.8400,  -84.4100,  -63.9400,\n",
            "          -97.1100,  -73.3500, -111.0600,  -83.9800,    0.0000,    0.0000,\n",
            "            0.0000,    1.0000],\n",
            "        [ -44.5000,  -35.2700,  -66.6000,  -50.9800,  -80.7900,  -61.2400,\n",
            "          -93.3400,  -70.6700, -106.1800,  -80.4500,    0.0000,    0.0000,\n",
            "            0.0000,    1.0000],\n",
            "        [ -44.3400,  -34.9700,  -66.5800,  -50.9300,  -78.0700,  -59.2700,\n",
            "          -89.9500,  -68.1800, -101.4600,  -77.0600,    0.0000,    0.0000,\n",
            "            0.0000,    1.0000],\n",
            "        [ -44.8000,  -35.1700,  -66.2000,  -50.4200,  -76.8000,  -58.2600,\n",
            "          -87.0900,  -65.5500,  -98.1400,  -73.8900,    0.0000,    0.0000,\n",
            "            0.0000,    1.0000],\n",
            "        [ -52.2200,  -41.6700,  -75.7400,  -57.7000, -109.1000,  -83.4200,\n",
            "         -149.6200, -114.9600, -197.3500, -151.9200,    0.0000,    0.0000,\n",
            "            0.0000,    0.0000],\n",
            "        [ -54.1500,  -42.9100,  -77.4300,  -59.3300, -112.0400,  -86.1600,\n",
            "         -155.0400, -119.5600, -205.7700, -159.3000,    0.0000,    0.0000,\n",
            "            0.0000,    0.0000],\n",
            "        [ -54.7200,  -43.0400,  -78.1900,  -60.0500, -113.3600,  -87.5000,\n",
            "         -157.6800, -121.9500, -207.9600, -161.6000,    0.0000,    0.0000,\n",
            "            0.0000,    0.0000],\n",
            "        [ -54.5400,  -42.6100,  -78.6700,  -60.5900, -114.3100,  -88.4400,\n",
            "         -158.3500, -122.7700, -207.1500, -161.3700,    0.0000,    0.0000,\n",
            "            0.0000,    0.0000],\n",
            "        [ -54.4700,  -41.8700,  -79.1500,  -60.5800, -115.7200,  -89.2600,\n",
            "         -159.2300, -122.9500, -200.9800, -157.1800,    0.0000,    0.0000,\n",
            "            0.0000,    0.0000]])\n",
            "Output: \n",
            "tensor([[0.0001],\n",
            "        [0.0001],\n",
            "        [0.0001],\n",
            "        [0.0001],\n",
            "        [0.0001],\n",
            "        [0.0009],\n",
            "        [0.0048],\n",
            "        [0.0049],\n",
            "        [0.0049],\n",
            "        [0.0049],\n",
            "        [0.0049],\n",
            "        [0.0049],\n",
            "        [0.0049],\n",
            "        [0.0049],\n",
            "        [0.0049],\n",
            "        [0.0049],\n",
            "        [0.0049],\n",
            "        [0.0049],\n",
            "        [0.0049],\n",
            "        [0.0049],\n",
            "        [0.0001],\n",
            "        [0.0001],\n",
            "        [0.0001],\n",
            "        [0.0001],\n",
            "        [0.0006]])\n"
          ]
        }
      ],
      "source": [
        "NN = Neural_Network()\n",
        "for i in range(500):  # entrenar la red 500 veces\n",
        "    print (\"#\" + str(i) + \" Loss: \" + str(torch.mean((y - NN(X))**2).detach().item()))  # mean sum squared loss\n",
        "    NN.train(X, y)\n",
        "NN.saveWeights(NN)\n",
        "NN.predict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "tM2w9qIYvHr1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}